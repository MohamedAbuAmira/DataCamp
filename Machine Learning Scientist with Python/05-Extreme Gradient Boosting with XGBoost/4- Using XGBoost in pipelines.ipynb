{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your XGBoost skills to the next level by incorporating your models into two end-to-end machine learning pipelines. You'll learn how to tune the most important XGBoost hyperparameters efficiently within a pipeline, and get an introduction to some more advanced preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Review of pipelines using sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Exploratory data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Before diving into the nitty gritty of pipelines and preprocessing, let&apos;s do some exploratory analysis of the original, unprocessed <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">Ames housing dataset</a>. When you worked with this data in previous chapters, we preprocessed it for you so you could focus on the core XGBoost concepts. In this chapter, you&apos;ll do the preprocessing yourself!</p>\n",
    "<p>A smaller version of this original, unprocessed dataset has been pre-loaded into a <code>pandas</code> DataFrame called <code>df</code>. Your task is to explore <code>df</code> in the Shell and pick the option that is <strong>incorrect</strong>. The larger purpose of this exercise is to understand the kinds of transformations you will need to perform in order to be able to use XGBoost.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 21 columns):\n",
      "MSSubClass      1460 non-null int64\n",
      "MSZoning        1460 non-null object\n",
      "LotFrontage     1201 non-null float64\n",
      "LotArea         1460 non-null int64\n",
      "Neighborhood    1460 non-null object\n",
      "BldgType        1460 non-null object\n",
      "HouseStyle      1460 non-null object\n",
      "OverallQual     1460 non-null int64\n",
      "OverallCond     1460 non-null int64\n",
      "YearBuilt       1460 non-null int64\n",
      "Remodeled       1460 non-null int64\n",
      "GrLivArea       1460 non-null int64\n",
      "BsmtFullBath    1460 non-null int64\n",
      "BsmtHalfBath    1460 non-null int64\n",
      "FullBath        1460 non-null int64\n",
      "HalfBath        1460 non-null int64\n",
      "BedroomAbvGr    1460 non-null int64\n",
      "Fireplaces      1460 non-null int64\n",
      "GarageArea      1460 non-null int64\n",
      "PavedDrive      1460 non-null object\n",
      "SalePrice       1460 non-null int64\n",
      "dtypes: float64(1), int64(15), object(5)\n",
      "memory usage: 239.7+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# load data\n",
    "df = pd.read_csv('datasets/ames_unprocessed_data.csv')\n",
    "\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>1515.463699</td>\n",
       "      <td>0.425342</td>\n",
       "      <td>0.057534</td>\n",
       "      <td>1.565068</td>\n",
       "      <td>0.382877</td>\n",
       "      <td>2.866438</td>\n",
       "      <td>0.613014</td>\n",
       "      <td>472.980137</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>0.499629</td>\n",
       "      <td>525.480383</td>\n",
       "      <td>0.518911</td>\n",
       "      <td>0.238753</td>\n",
       "      <td>0.550916</td>\n",
       "      <td>0.502885</td>\n",
       "      <td>0.815778</td>\n",
       "      <td>0.644666</td>\n",
       "      <td>213.804841</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1129.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.500000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1776.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5642.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        MSSubClass  LotFrontage        LotArea  OverallQual  OverallCond  \\\n",
       "count  1460.000000  1201.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     56.897260    70.049958   10516.828082     6.099315     5.575342   \n",
       "std      42.300571    24.284752    9981.264932     1.382997     1.112799   \n",
       "min      20.000000    21.000000    1300.000000     1.000000     1.000000   \n",
       "25%      20.000000    59.000000    7553.500000     5.000000     5.000000   \n",
       "50%      50.000000    69.000000    9478.500000     6.000000     5.000000   \n",
       "75%      70.000000    80.000000   11601.500000     7.000000     6.000000   \n",
       "max     190.000000   313.000000  215245.000000    10.000000     9.000000   \n",
       "\n",
       "         YearBuilt    Remodeled    GrLivArea  BsmtFullBath  BsmtHalfBath  \\\n",
       "count  1460.000000  1460.000000  1460.000000   1460.000000   1460.000000   \n",
       "mean   1971.267808     0.476712  1515.463699      0.425342      0.057534   \n",
       "std      30.202904     0.499629   525.480383      0.518911      0.238753   \n",
       "min    1872.000000     0.000000   334.000000      0.000000      0.000000   \n",
       "25%    1954.000000     0.000000  1129.500000      0.000000      0.000000   \n",
       "50%    1973.000000     0.000000  1464.000000      0.000000      0.000000   \n",
       "75%    2000.000000     1.000000  1776.750000      1.000000      0.000000   \n",
       "max    2010.000000     1.000000  5642.000000      3.000000      2.000000   \n",
       "\n",
       "          FullBath     HalfBath  BedroomAbvGr   Fireplaces   GarageArea  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1460.000000  1460.000000   \n",
       "mean      1.565068     0.382877      2.866438     0.613014   472.980137   \n",
       "std       0.550916     0.502885      0.815778     0.644666   213.804841   \n",
       "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000      2.000000     0.000000   334.500000   \n",
       "50%       2.000000     0.000000      3.000000     1.000000   480.000000   \n",
       "75%       2.000000     1.000000      3.000000     1.000000   576.000000   \n",
       "max       3.000000     2.000000      8.000000     3.000000  1418.000000   \n",
       "\n",
       "           SalePrice  \n",
       "count    1460.000000  \n",
       "mean   180921.195890  \n",
       "std     79442.502883  \n",
       "min     34900.000000  \n",
       "25%    129975.000000  \n",
       "50%    163000.000000  \n",
       "75%    214000.000000  \n",
       "max    755000.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Well done! The LotFrontage column actually does have missing values: 259, to be precise. Additionally, notice how columns such as MSZoning, PavedDrive, and HouseStyle are categorical. These need to be encoded numerically before you can use XGBoost. This is what you'll do in the coming exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Encoding categorical columns I: LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that you&apos;ve seen what will need to be done to get the housing data ready for XGBoost, let&apos;s go through the process step-by-step. </p>\n",
    "<p>First, you will need to fill in missing values - as you saw previously, the column <code>LotFrontage</code> has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. You can watch <a href=\"https://campus.datacamp.com/courses/supervised-learning-with-scikit-learn/preprocessing-and-pipelines?ex=1\">this video</a> from <a href=\"https://www.datacamp.com/courses/supervised-learning-with-scikit-learn\">Supervised Learning with scikit-learn</a> for a refresher on the idea. </p>\n",
    "<p>The data has five categorical columns: <code>MSZoning</code>, <code>PavedDrive</code>, <code>Neighborhood</code>, <code>BldgType</code>, and <code>HouseStyle</code>. Scikit-learn has a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\">LabelEncoder</a> function that converts the values in each categorical column into integers. You&apos;ll practice using this here.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import <code>LabelEncoder</code> from <code>sklearn.preprocessing</code>.</li>\n",
    "<li>Fill in missing values in the <code>LotFrontage</code> column with <code>0</code> using <code>.fillna()</code>.</li>\n",
    "<li>Create a boolean mask for categorical columns. You can do this by checking for whether <code>df.dtypes</code> equals <code>object</code>.</li>\n",
    "<li>Create a <code>LabelEncoder</code> object. You can do this in the same way you instantiate any scikit-learn estimator. </li>\n",
    "<li>Encode all of the categorical columns into integers using <code>LabelEncoder()</code>. To do this, use the <code>.fit_transform()</code> method of <code>le</code> in the provided lambda function.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df['LotFrontage'].fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Well done! Notice how the entries in each categorical column are now encoded numerically. A BldgTpe of 1Fam is encoded as 0, while a HouseStyle of 2Story is encoded as 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Encoding categorical columns II: OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using <code>LabelEncoder</code>, the <code>CollgCr</code> <code>Neighborhood</code> was encoded as <code>5</code>, while the <code>Veenker</code> <code>Neighborhood</code> was encoded as <code>24</code>, and <code>Crawfor</code> as <code>6</code>. Is <code>Veenker</code> &quot;greater&quot; than <code>Crawfor</code> and <code>CollgCr</code>? No - and allowing the model to assume this natural ordering may result in poor performance.</p>\n",
    "<p>As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or &quot;dummy&quot; variables. You can do this using scikit-learn&apos;s <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">OneHotEncoder</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import <code>OneHotEncoder</code> from <code>sklearn.preprocessing</code>.</li>\n",
    "<li>Instantiate a <code>OneHotEncoder</code> object called <code>ohe</code>. Specify the keyword arguments <code>categorical_features=categorical_mask</code> and <code>sparse=False</code>.</li>\n",
    "<li>Using its <code>.fit_transform()</code> method, apply the <code>OneHotEncoder</code> to <code>df</code> and save the result as <code>df_encoded</code>. The output will be a NumPy array.</li>\n",
    "<li>Print the first 5 rows of <code>df_encoded</code>, and then the shape of <code>df</code> as well as <code>df_encoded</code> to compare the difference.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03\n",
      "  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03\n",
      "  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n",
      "  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.800e+01 1.125e+04\n",
      "  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 7.000e+01 6.000e+01 9.550e+03\n",
      "  7.000e+00 5.000e+00 1.915e+03 1.000e+00 1.717e+03 1.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 3.000e+00 1.000e+00 6.420e+02 1.400e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 8.400e+01 1.426e+04\n",
      "  8.000e+00 5.000e+00 2.000e+03 0.000e+00 2.198e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 4.000e+00 1.000e+00 8.360e+02 2.500e+05]]\n",
      "(1460, 21)\n",
      "(1460, 62)\n"
     ]
    }
   ],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ohe.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Encoding categorical columns III: DictVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Alright, one final trick before you dive into pipelines. The two step process you just went through - <code>LabelEncoder</code> followed by <code>OneHotEncoder</code> - can be simplified by using a <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\">DictVectorizer</a>. </p>\n",
    "<p>Using a <code>DictVectorizer</code> on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go. </p>\n",
    "<p>Your task is to work through this strategy in this exercise!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import <code>DictVectorizer</code> from <code>sklearn.feature_extraction</code>.</li>\n",
    "<li>Convert <code>df</code> into a dictionary called <code>df_dict</code> using its <code>.to_dict()</code> method with <code>&quot;records&quot;</code> as the argument.</li>\n",
    "<li>Instantiate a <code>DictVectorizer</code> object called <code>dv</code> with the keyword argument <code>sparse=False</code>.</li>\n",
    "<li>Apply the <code>DictVectorizer</code> on <code>df_dict</code> by using its <code>.fit_transform()</code> method.</li>\n",
    "<li>Hit &apos;Submit Answer&apos; to print the resulting first five rows and the vocabulary.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
      "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
      "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
      "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
      "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
      "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
      "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
      "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
      "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
     ]
    }
   ],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict('records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Fantastic! Besides simplifying the process into one step, DictVectorizer has useful attributes such as vocabulary_ which maps the names of the features to their indices. With the data preprocessed, it's time to move onto pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Preprocessing within a pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that you&apos;ve seen what steps need to be taken individually to properly process the Ames housing data, let&apos;s use the much cleaner and more succinct <code>DictVectorizer</code> approach and put it alongside an <code>XGBoostRegressor</code> inside of a scikit-learn pipeline.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import <code>DictVectorizer</code> from <code>sklearn.feature_extraction</code> and <code>Pipeline</code> from <code>sklearn.pipeline</code>.</li>\n",
    "<li>Fill in any missing values in the <code>LotFrontage</code> column of <code>X</code> with <code>0</code>.</li>\n",
    "<li>Complete the steps of the pipeline with <code>DictVectorizer(sparse=False)</code> for <code>&quot;ohe_onestep&quot;</code> and <code>xgb.XGBRegressor()</code> for <code>&quot;xgb_model&quot;</code>.</li>\n",
    "<li>Create the pipeline using <code>Pipeline()</code> and <code>steps</code>.</li>\n",
    "<li>Fit the <code>Pipeline</code>. Don&apos;t forget to convert <code>X</code> into a format that <code>DictVectorizer</code> understands by calling the <code>to_dict(&quot;records&quot;)</code> method on <code>X</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:57:30] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('ohe_onestep',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0,\n",
       "                              importance_type='gain', learning_rate=0.1,\n",
       "                              max_delta_step=0, max_depth=3, min_child_weight=1,\n",
       "                              missing=None, n_estimators=100, n_jobs=1,\n",
       "                              nthread=None, objective='reg:linear',\n",
       "                              random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                              scale_pos_weight=1, seed=None, silent=None,\n",
       "                              subsample=1, verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "X=df.drop('SalePrice', axis=1)\n",
    "y=df['SalePrice']\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict(\"records\"), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7- Incorporating XGBoost into pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8- Cross-validating your XGBoost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Create a pipeline called <code>xgb_pipeline</code> using <code>steps</code>.</li>\n",
    "<li>Perform 10-fold cross-validation using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\"><code>cross_val_score()</code></a>. You&apos;ll have to pass in the pipeline, <code>X</code> (as a dictionary, using <code>.to_dict(&quot;records&quot;)</code>), <code>y</code>, the number of folds you want to use, and <code>scoring</code> (<code>&quot;neg_mean_squared_error&quot;</code>). </li>\n",
    "<li>Print the 10-fold RMSE.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:07] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\MohammedTaysser\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:08:08] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[21:08:09] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "10-fold RMSE:  30343.486551766466\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:linear\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict(\"records\"), y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9- Kidney disease case study I: Categorical Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>You&apos;ll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The <a href=\"https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\">chronic kidney disease dataset</a> contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.</p>\n",
    "<p>As Sergey mentioned in the video, you&apos;ll be introduced to a new library, <a href=\"https://github.com/pandas-dev/sklearn-pandas\"><code>sklearn_pandas</code></a>, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you&apos;ll be able to impute missing categorical values directly using the <code>Categorical_Imputer()</code> class in <code>sklearn_pandas</code>, and the <code>DataFrameMapper()</code> class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.</p>\n",
    "<p>We&apos;ve also created a transformer called a <code>Dictifier</code> that encapsulates converting a DataFrame using <code>.to_dict(&quot;records&quot;)</code> without you having to do it explicitly (and so that it works in a pipeline). Finally, we&apos;ve also provided the list of feature names in <code>kidney_feature_names</code>, the target name in <code>kidney_target_name</code>, the features in <code>X</code>, and the target in <code>y</code>.</p>\n",
    "<p>In this exercise, your task is to apply the <code>CategoricalImputer</code> to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments <code>input_df=True</code> and <code>df_out=True</code>? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a <code>numpy</code> array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with <code>numpy</code> arrays, not <code>pandas</code> DataFrames, even though their basic indexing interfaces are similar.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 401 entries, id to 399\n",
      "Data columns (total 25 columns):\n",
      "age      392 non-null object\n",
      "bp       389 non-null object\n",
      "sg       354 non-null object\n",
      "al       355 non-null object\n",
      "su       352 non-null object\n",
      "bgr      249 non-null object\n",
      "bu       336 non-null object\n",
      "sc       397 non-null object\n",
      "sod      397 non-null object\n",
      "pot      357 non-null object\n",
      "hemo     382 non-null object\n",
      "pcv      384 non-null object\n",
      "wc       314 non-null object\n",
      "rc       313 non-null object\n",
      "rbc      349 non-null object\n",
      "pc       331 non-null object\n",
      "pcc      296 non-null object\n",
      "ba       271 non-null object\n",
      "htn      399 non-null object\n",
      "dm       399 non-null object\n",
      "cad      399 non-null object\n",
      "appet    400 non-null object\n",
      "pe       400 non-null object\n",
      "ane      400 non-null object\n",
      "class    401 non-null object\n",
      "dtypes: object(25)\n",
      "memory usage: 81.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#importing and processing \n",
    "import pandas as pd\n",
    "names=['age',  'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', 'hemo',\n",
    " 'pcv', 'wc', 'rc', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', 'cad', 'appet', 'pe',\n",
    " 'ane', 'class']\n",
    "\n",
    "kidney_data = pd.read_csv('datasets/kidney_disease_unprocessed_kaggle.csv', names=names)\n",
    "X, y = kidney_data.iloc[:,:-1], kidney_data.iloc[:,-1]\n",
    "\n",
    "kidney_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age        9\n",
      "bp        12\n",
      "sg        47\n",
      "al        46\n",
      "su        49\n",
      "bgr      152\n",
      "bu        65\n",
      "sc         4\n",
      "sod        4\n",
      "pot       44\n",
      "hemo      19\n",
      "pcv       17\n",
      "wc        87\n",
      "rc        88\n",
      "rbc       52\n",
      "pc        70\n",
      "pcc      105\n",
      "ba       130\n",
      "htn        2\n",
      "dm         2\n",
      "cad        2\n",
      "appet      1\n",
      "pe         1\n",
      "ane        1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper(\n",
    "                                            [([numeric_feature],Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
    "                                            input_df=True,\n",
    "                                            df_out=True\n",
    "                                           )\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper(\n",
    "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
    "                                                input_df=True,\n",
    "                                                df_out=True\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 25 columns):\n",
      "age      400 non-null object\n",
      "bp       400 non-null object\n",
      "sg       400 non-null object\n",
      "al       400 non-null object\n",
      "su       400 non-null object\n",
      "bgr      400 non-null object\n",
      "bu       400 non-null object\n",
      "sc       400 non-null object\n",
      "sod      400 non-null object\n",
      "pot      400 non-null object\n",
      "hemo     400 non-null object\n",
      "pcv      400 non-null object\n",
      "wc       400 non-null object\n",
      "rc       400 non-null object\n",
      "rbc      400 non-null object\n",
      "pc       400 non-null object\n",
      "pcc      400 non-null object\n",
      "ba       400 non-null object\n",
      "htn      400 non-null object\n",
      "dm       400 non-null object\n",
      "cad      400 non-null object\n",
      "appet    400 non-null object\n",
      "pe       400 non-null object\n",
      "ane      400 non-null object\n",
      "class    400 non-null object\n",
      "dtypes: object(25)\n",
      "memory usage: 78.2+ KB\n"
     ]
    }
   ],
   "source": [
    "kidney_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10- Kidney disease case study II: Feature Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn&apos;s <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html\">FeatureUnion</a> to concatenate their results, which are contained in two separate transformer objects - <code>numeric_imputation_mapper</code>, and <code>categorical_imputation_mapper</code>, respectively.</p>\n",
    "<p>You may have already encountered <code>FeatureUnion</code> in <a href=\"https://campus.datacamp.com/courses/machine-learning-with-the-experts-school-budgets/improving-your-model?ex=7\">Machine Learning with the Experts: School Budgets</a>. Just like with pipelines, you have to pass it a list of <code>(string, transformer)</code> tuples, where the first half of each tuple is the name of the transformer.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Import <code>FeatureUnion</code> from <code>sklearn.pipeline</code>.</li>\n",
    "<li>Combine the results of <code>numeric_imputation_mapper</code> and <code>categorical_imputation_mapper</code> using <code>FeatureUnion()</code>, with the names <code>&quot;num_mapper&quot;</code> and <code>&quot;cat_mapper&quot;</code> respectively.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([\n",
    "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11- Kidney disease case study III: Full pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It&apos;s time to piece together all of the transforms along with an <code>XGBClassifier</code> to build the full pipeline!</p>\n",
    "<p>Besides the <code>numeric_categorical_union</code> that you created in the previous exercise, there are two other transforms needed: the <code>Dictifier()</code> transform which we created for you, and the <code>DictVectorizer()</code>. </p>\n",
    "<p>After creating the pipeline, your task is to cross-validate it to see how well it performs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Create the pipeline using the <code>numeric_categorical_union</code>, <code>Dictifier()</code>, and <code>DictVectorizer(sort=False)</code> transforms, and <code>xgb.XGBClassifier()</code> estimator with <code>max_depth=3</code>. Name the transforms <code>&quot;featureunion&quot;</code>, <code>&quot;dictifier&quot;</code> <code>&quot;vectorizer&quot;</code>, and the estimator <code>&quot;clf&quot;</code>.</li>\n",
    "<li>Perform 3-fold cross-validation on the <code>pipeline</code> using <code>cross_val_score()</code>. Pass it the pipeline, <code>pipeline</code>, the features, <code>kidney_data</code>, the outcomes, <code>y</code>. Also set <code>scoring</code> to <code>&quot;roc_auc&quot;</code> and <code>cv</code> to <code>3</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([\n",
    "                     (\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
    "                    ])\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12- Bringing it all together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Alright, it&apos;s time to bring together everything you&apos;ve learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.</p>\n",
    "<p>Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>Set up the parameter grid to tune <code>&apos;clf__learning_rate&apos;</code> (from <code>0.05</code> to <code>1</code> in increments of <code>0.05</code>), <code>&apos;clf__max_depth&apos;</code> (from <code>3</code> to <code>10</code> in increments of <code>1</code>), and <code>&apos;clf__n_estimators&apos;</code> (from <code>50</code> to <code>200</code> in increments of <code>50</code>).</li>\n",
    "<li>Using your <code>pipeline</code> as the estimator, perform 2-fold <code>RandomizedSearchCV</code> with an <code>n_iter</code> of <code>2</code>. Use <code>&quot;roc_auc&quot;</code> as the metric, and set <code>verbose</code> to <code>1</code> so the output is more detailed. Store the result in <code>randomized_roc_auc</code>.</li>\n",
    "<li>Fit <code>randomized_roc_auc</code> to <code>X</code> and <code>y</code>.</li>\n",
    "<li>Compute the best score and best estimator of <code>randomized_roc_auc</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(.05, 1, .05),\n",
    "    'clf__max_depth': np.arange(3,10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,\n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)\n",
    "\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Amazing work! This type of pipelining is very common in real-world data science and you're well on your way towards mastering it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
